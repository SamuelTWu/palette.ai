{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, EvalPrediction\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_decimal(hex_num):\n",
    "    try:\n",
    "        # Convert the hexadecimal string to decimal\n",
    "        decimal_num = int(hex_num, 16)\n",
    "        return decimal_num\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def display_random_palette(df):\n",
    "    # Select a random row\n",
    "    random_row = df.sample().iloc[0]\n",
    "\n",
    "    # Extract the description and colors\n",
    "    description = random_row[\"description\"]\n",
    "    colors = [random_row[\"color1\"], random_row[\"color2\"], random_row[\"color3\"], random_row[\"color4\"], random_row[\"color5\"]]\n",
    "\n",
    "    # Output the description\n",
    "    print(f\"Description: {description}\")\n",
    "\n",
    "    # Display the colors as a horizontal bar\n",
    "    fig, ax = plt.subplots(figsize=(8, 2))\n",
    "    for i, color in enumerate(colors):\n",
    "        ax.add_patch(plt.Rectangle((i, 0), 1, 1, color=color))\n",
    "\n",
    "    ax.set_xlim(0, 5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def convert_df(df):\n",
    "    #converts the csv from columns of hex to 3 columns of RGB in dec\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[\"description\"] = df[\"description\"]\n",
    "    for color_col in [col for col in df.columns if col != 'description']:\n",
    "        R = []\n",
    "        G = []\n",
    "        B = []\n",
    "        for color in df[color_col]:\n",
    "            #given a col, split into R,G,B\n",
    "            color = color[1:]\n",
    "            R.append((hex_to_decimal(color[0:2]))/255)\n",
    "            G.append((hex_to_decimal(color[2:4]))/255)\n",
    "            B.append((hex_to_decimal(color[4:6]))/255)\n",
    "        #add the new columns \n",
    "        new_df[color_col+\"_R\"] = R\n",
    "        new_df[color_col+\"_G\"] = G\n",
    "        new_df[color_col+\"_B\"] = B\n",
    "    return new_df\n",
    "\n",
    "def organize_colors(df):\n",
    "    for index, row in df.iterrows():\n",
    "        colors = []\n",
    "        color_dict = {}\n",
    "        for color in row[1:]:\n",
    "            color_values = color[1:]\n",
    "            R = hex_to_decimal(color_values[0:2])\n",
    "            G = hex_to_decimal(color_values[2:4])\n",
    "            B = hex_to_decimal(color_values[4:6])\n",
    "            colors.append((R,G,B))\n",
    "            color_dict[(R,G,B)] = color\n",
    "        lighest = max(colors, key=lambda x: sum(x))\n",
    "        colors.remove(lighest)\n",
    "        darkest = min(colors, key=lambda x: sum(x))\n",
    "        colors.remove(darkest)\n",
    "        sorted_tuples = sorted(colors, key=lambda x: (x[0], x[1], x[2]), reverse=True)\n",
    "        df.at[index, 'color1'] = color_dict[lighest]\n",
    "        df.at[index, 'color2'] = color_dict[sorted_tuples[0]]\n",
    "        df.at[index, 'color3'] = color_dict[sorted_tuples[1]]\n",
    "        df.at[index, 'color4'] = color_dict[sorted_tuples[2]]\n",
    "        df.at[index, 'color5'] = color_dict[darkest]\n",
    "    return df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"palette_hex.csv\")\n",
    "new_df = organize_colors(df)\n",
    "new_df.to_csv(\"palette_ordered.csv\", index = False)\n",
    "df = pd.read_csv(\"palette_ordered.csv\")\n",
    "new_df = convert_df(df)\n",
    "new_df.to_csv(\"palette_dec.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='palette_dec.csv')\n",
    "labels = [label for label in dataset['train'].features.keys() if label not in ['description']]\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/restricted/projectnb/cifulab/Sam/website/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    text = examples[\"description\"]\n",
    "    encoding = tokenizer(text, truncation=True, padding=\"max_length\", max_length=10)\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    # Ensure labels remain as NumPy array\n",
    "    encoding[\"labels\"] = labels_matrix\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beba67b61b3f4941957978329c30b856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7163dcbd2db147038c6e1777bec0689d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c9931a4fe54ac5ba27a8728d8f8a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=[] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"rmse\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"color_model/\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=40,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"rmse\":rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_for_regression\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/restricted/projectnb/cifulab/Sam/website/.venv/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='519' max='20840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  519/20840 00:12 < 08:07, 41.71 it/s, Epoch 0.99/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4488574266433716,\n",
       " 'eval_mse': 8.855853080749512,\n",
       " 'eval_mae': 2.176762104034424,\n",
       " 'eval_rmse': 2.9758784770965576,\n",
       " 'eval_runtime': 0.2556,\n",
       " 'eval_samples_per_second': 2038.476,\n",
       " 'eval_steps_per_second': 258.233,\n",
       " 'epoch': 40.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a collection of filters for the logits\n",
    "def normalize_list(numbers):\n",
    "            min_val = min(numbers)\n",
    "            max_val = max(numbers)\n",
    "            normalized_numbers = []\n",
    "            for num in numbers:\n",
    "                normalized_num = (num - min_val) / (max_val - min_val)\n",
    "                normalized_numbers.append(normalized_num)\n",
    "            return normalized_numbers\n",
    "def clamp_list(numbers):\n",
    "  return [max(0, min(num, 1)) for num in numbers]\n",
    "def sigmoid_list(numbers):\n",
    "  return [(1/(1+np.exp(-float(x)))) for x in numbers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_color(output):\n",
    "    colors = []\n",
    "    logits = sigmoid_list(output.logits.squeeze())\n",
    "    for i in range(0,len(logits),3):\n",
    "        R = float(logits[i])\n",
    "        G = float(logits[i+1])\n",
    "        B = float(logits[i+2])\n",
    "        print(R,G,B)\n",
    "        hex_code = \"#\"+str(hex(int(R*255))[2:].zfill(2))+str(hex(int(G*255))[2:].zfill(2))+str(hex(int(B*255))[2:].zfill(2))\n",
    "        colors.append(hex_code)\n",
    "    return colors\n",
    "def display_colors(colors):\n",
    "    # Display the colors as a horizontal bar\n",
    "    fig, ax = plt.subplots(figsize=(8, 2))\n",
    "    for i, color in enumerate(colors):\n",
    "        ax.add_patch(plt.Rectangle((i, 0), 1, 1, color=color))\n",
    "    ax.set_xlim(0, 5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for if you have to load a model\n",
    "model_directory = \"models\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_directory)\n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847144161343961 0.9464854621042599 0.9047058830410717\n",
      "0.9962625616289568 0.7282467347145419 0.05625517750042631\n",
      "0.9826090719277193 0.4292107564028588 0.255985797072936\n",
      "0.9709429411977035 0.2177630861012605 0.012386391777274732\n",
      "0.0497648906231494 0.010237690949466242 0.012017406842802532\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAACuCAYAAACm9LxMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADr0lEQVR4nO3dsU0DQQBFwbVluqEASqBrGiBA0AKCCCFBADZnLwVA4AQb6c3EG/xo9e6SXc055wAAIGN97gEAAJyWAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAzObYg8vb81/u4D/ZPo313fVYzd25l3AC8+Ni7G8uxzj4HiyY289xuL8dYx7OPYUTeNyOcfUwxtabXxnv++Woc258flpexV/JbiP+SpYv8Rfysog/fufWBwCIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAEDMas45zz0CAIDT8QcQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAg5hu6hzMNzL+SSQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"pikachu\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "prediction = trainer.model(**encoding)\n",
    "display_colors(output_to_color(prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de26e5a7eefe4525ac0ef7d5aaf48734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Palette AI\" #@param {type:\"string\"}\n",
    "\n",
    "description = \"\"\"\n",
    "A Sam Wu Creation\n",
    "\n",
    "Check out the GitHub at \n",
    "\"\"\"\n",
    "task_name = \"Image Classification\"\n",
    "task_type = 'image-classification'\n",
    "metric_name = 'Accuracy'\n",
    "metric_type = 'accuracy'\n",
    "metric_value = trainer.callback_metrics['val_acc'].item()\n",
    "\n",
    "# Delete model folder, as we (re)create it here.\n",
    "if Path('./model').exists():\n",
    "    shutil.rmtree('./model')\n",
    "\n",
    "token = HfFolder().get_token()\n",
    "if not token:\n",
    "    raise RuntimeError(\"You must log in to push to hub! Run notebook_login() in the cell above.\")\n",
    "\n",
    "hf_username = HfApi().whoami()['name']\n",
    "model_url = HfApi().create_repo(token=token, repo_id=model_id, exist_ok=True)\n",
    "model_repo = Repository(\"./model\", clone_from=model_url, use_auth_token=token, git_email=f\"{hf_username}@users.noreply.huggingface.co\", git_user=hf_username)\n",
    "model.save_pretrained(model_repo.local_dir)\n",
    "feature_extractor.save_pretrained(model_repo.local_dir)\n",
    "\n",
    "# Copy over tensorboard logs from lightning_logs/ into ./model/runs/\n",
    "tensorboard_logs_path = next(Path(trainer.logger.log_dir).glob('events.out*'))\n",
    "model_repo_logs_path = Path(model_repo.local_dir) / 'runs'\n",
    "model_repo_logs_path.mkdir(exist_ok=True, parents=True)\n",
    "shutil.copy(tensorboard_logs_path, model_repo_logs_path)\n",
    "\n",
    "# Copy over a few example images\n",
    "example_images_dir = Path(model_repo.local_dir) / 'images'\n",
    "example_images_dir.mkdir(exist_ok=True, parents=True)\n",
    "image_markdown_template = '''\n",
    "#### {class_name}\n",
    "\n",
    "![{class_name}](images/{example_image_path})\n",
    "'''\n",
    "example_images_markdown = \"\"\n",
    "for class_idx, class_name in enumerate(ds.classes):\n",
    "    folder = ds.root / class_name\n",
    "    image_path = sorted(folder.glob('*'))[0]\n",
    "    example_image_path = example_images_dir / f\"{class_name.replace(' ', '_')}{image_path.suffix}\"\n",
    "    shutil.copy(image_path, example_image_path)\n",
    "    example_images_markdown += image_markdown_template.format(\n",
    "        class_name=class_name,\n",
    "        example_image_path=example_image_path.name\n",
    "    )\n",
    "\n",
    "\n",
    "# Prepare README.md from information gathered above\n",
    "readme_txt = f\"\"\"\n",
    "---\n",
    "tags:\n",
    "- image-classification\n",
    "- pytorch\n",
    "- huggingpics\n",
    "metrics:\n",
    "- {metric_type}\n",
    "\n",
    "model-index:\n",
    "- name: {model_id}\n",
    "  results:\n",
    "  - task:\n",
    "      name: {task_name}\n",
    "      type: {task_type}\n",
    "    metrics:\n",
    "      - name: {metric_name}\n",
    "        type: {metric_type}\n",
    "        value: {metric_value}\n",
    "---\n",
    "\n",
    "# {model_id}\n",
    "\n",
    "{description}\n",
    "\n",
    "## Example Images\n",
    "\n",
    "{example_images_markdown}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "(Path(model_repo.local_dir) / 'README.md').write_text(readme_txt)\n",
    "\n",
    "commit_url = model_repo.push_to_hub()\n",
    "\n",
    "print(\"Check out your model at:\")\n",
    "print(f\"https://huggingface.co/{hf_username}/{model_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
